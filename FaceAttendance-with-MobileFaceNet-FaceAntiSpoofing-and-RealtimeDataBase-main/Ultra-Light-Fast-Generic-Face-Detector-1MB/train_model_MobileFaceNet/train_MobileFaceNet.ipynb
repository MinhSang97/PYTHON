{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Process Data\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WllCH3YgeYCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Hi16kheS4d"
      },
      "outputs": [],
      "source": [
        "pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "I9E0GHMlfYI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d ntl0601/casia-webface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSZPjD9HfdjP",
        "outputId": "d15cee48-74c3-4308-ec43-b70eded0b114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading casia-webface.zip to /content\n",
            "100% 2.53G/2.53G [02:07<00:00, 24.2MB/s]\n",
            "100% 2.53G/2.53G [02:07<00:00, 21.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "Casia_path = \"/content/casia-webface.zip\"\n",
        "with ZipFile(Casia_path, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Done unzipping CASIA-WebFace.zip')"
      ],
      "metadata": {
        "id": "ANop92QhfqEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c58f557-fd37-40cc-fa11-22ea2f076966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done unzipping CASIA-WebFace.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MobileFaceNet\n",
        "!git clone https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTWrEIEggt2L",
        "outputId": "566e8391-04d5-4fcc-d7f1-3b5431db9c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MobileFaceNet\n",
            "Cloning into 'Ultra-Light-Fast-Generic-Face-Detector-1MB'...\n",
            "remote: Enumerating objects: 953, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 953 (delta 120), reused 104 (delta 104), pack-reused 784\u001b[K\n",
            "Receiving objects: 100% (953/953), 37.29 MiB | 16.41 MiB/s, done.\n",
            "Resolving deltas: 100% (483/483), done.\n",
            "Checking out files: 100% (230/230), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MobileFaceNet/Ultra-Light-Fast-Generic-Face-Detector-1MB\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "F-NsxBWMmn5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo hàm xử lý ảnh --> Chuyển ảnh về đúng format của models\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import vision.utils.box_utils_numpy as box_utils\n",
        "import onnxruntime as ort\n",
        "\n",
        "\n",
        "def pre_process(img):\n",
        "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (320, 240)) # Đúng kích thước ảnh của mô hình RFB-320\n",
        "    image_mean = np.array([127, 127, 127])\n",
        "    image = (image - image_mean) / 128\n",
        "    image = np.transpose(image, [2, 0, 1]) # Thay đổi kích thước thành định dạng (batch size, chanel, height, weight)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image.astype(np.float32)\n",
        "    return image\n",
        "\n",
        "# Tạo hàm Non - maximum suppresion --> Bỏ các BBox không cần thiết\n",
        "def non_max_sup(width,height,confidences,boxes,prob_threshold, iou_threshold = 0.3, top_k = 1):\n",
        "    boxes = boxes[0]\n",
        "    confidences = confidences[0]\n",
        "\n",
        "    picked_box_probs,picked_labels = [],[]\n",
        "    for class_index in range(1, confidences.shape[1]):\n",
        "        \"\"\"Lấy ra những Confidence > prob_threshold (đk đầu vào)\"\"\"\n",
        "        probs = confidences[:, class_index]\n",
        "        mask = probs > prob_threshold # --> Bool\n",
        "        probs = probs[mask]\n",
        "\n",
        "        if probs.shape[0] == 0: continue\n",
        "        subset_boxes = boxes[mask,:]\n",
        "        box_probs = np.concatenate([subset_boxes,probs.reshape(-1,1)], axis = 1)\n",
        "        box_probs = box_utils.hard_nms(box_probs,iou_threshold,top_k,)\n",
        "        picked_box_probs.append(box_probs)\n",
        "        picked_labels.extend([class_index]*box_probs.shape[0])\n",
        "    if not picked_box_probs:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    \n",
        "    picked_box_probs = np.concatenate(picked_box_probs)\n",
        "    picked_box_probs[:,0] *= width\n",
        "    picked_box_probs[:,1] *= height\n",
        "    picked_box_probs[:,2] *= width\n",
        "    picked_box_probs[:,3] *= height\n",
        "    return picked_box_probs[:,:4].astype(np.int32), np.array(picked_labels), picked_box_probs[:,4]"
      ],
      "metadata": {
        "id": "KZR_qxBnmgcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/casia-webface\"\n",
        "new_dataset = \"New_CASIA-WebFace\"\n",
        "\n",
        "list_folder = os.listdir(dataset_path)\n",
        "\n",
        "# Khởi tạo model ONNX\n",
        "onnx_path = \"models/onnx/version-RFB-320.onnx\"\n",
        "ort_session = ort.InferenceSession(onnx_path)\n",
        "input_name = ort_session.get_inputs()[0].name\n",
        "threshold = 0.7\n",
        "\n",
        "list_folder = os.listdir(dataset_path)\n",
        "\n",
        "for folder_path in list_folder:\n",
        "    new_data_path = os.path.join(new_dataset,folder_path)\n",
        "    if not os.path.exists(new_data_path):\n",
        "        os.makedirs(new_data_path)\n",
        "    \n",
        "    img_path = os.path.join(dataset_path, folder_path)\n",
        "\n",
        "    list_file = os.listdir(img_path)\n",
        "\n",
        "    for file_path in list_file:\n",
        "        image_path = os.path.join(img_path, file_path)\n",
        "        print('Image path: ', image_path)\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        image = pre_process(img)\n",
        "        confidences,boxes = ort_session.run(None, {input_name: image})\n",
        "        boxes, labels, probs = non_max_sup(img.shape[1], img.shape[0], confidences, boxes, threshold)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            cv2.imwrite(os.path.join(new_data_path, file_path), img)\n",
        "        \n",
        "        for i in range(boxes.shape[0]):\n",
        "            if boxes[i][1] < 0:\n",
        "                boxes[i][1] = 0\n",
        "            \n",
        "            if boxes[i][0] < 0:\n",
        "                boxes[i][0] = 0\n",
        "            \n",
        "            box = boxes[i,:]\n",
        "            max = boxes[0][2] * boxes[0][3]\n",
        "\n",
        "            if boxes[i][2] * boxes[i][3] >= max:\n",
        "                box = boxes[i]\n",
        "            else:\n",
        "                box = boxes[0]\n",
        "            \n",
        "            cv2.imwrite(os.path.join(new_data_path, file_path), img[box[1]:box[3], box[0]: box[2]])"
      ],
      "metadata": {
        "id": "J11MmOEZl_81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huấn luyện mô hình"
      ],
      "metadata": {
        "id": "Hc59-ol-FCL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "fC7UksBEnYqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d unkownhihi/casiawebface-cropped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjaUi03uFJ2t",
        "outputId": "ac094b44-5cc9-4b8b-9317-9906a3333d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading casiawebface-cropped.zip to /content\n",
            "100% 6.76G/6.76G [05:44<00:00, 23.0MB/s]\n",
            "100% 6.76G/6.76G [05:44<00:00, 21.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create file csv from dataset-crop"
      ],
      "metadata": {
        "id": "bdB35XByHnvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "Casia_path = \"/content/casiawebface-cropped.zip\"\n",
        "with ZipFile(Casia_path, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Done unzipping New-CASIA-WebFace.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vSSsIZiFw4Y",
        "outputId": "0735abe4-fbc4-4676-8ff5-aad61241e0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done unzipping New-CASIA-WebFace.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "root = 'CASIA-WebFace_cropped/'\n",
        "folder_name_list = os.listdir(root)\n",
        "folder_name_list.sort() \n",
        "print('Number of classes (or nuber of person ID): ', len(folder_name_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWISfPxzGMcy",
        "outputId": "e29c1a8d-5d65-4267-f194-e5315fca0c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes (or nuber of person ID):  10575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data_train.csv', 'a') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for i, folder_name in enumerate(folder_name_list):\n",
        "        folder_path = root + folder_name\n",
        "        img_name_list = os.listdir(folder_path)\n",
        "\n",
        "        for img_name in img_name_list:\n",
        "            img_path = os.path.join(root, folder_name, img_name)\n",
        "            label = i\n",
        "            writer.writerow([img_path, label])"
      ],
      "metadata": {
        "id": "ZrDMHNSRGYgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data_train.csv',names=[\"img_path\",\"label\"])\n",
        "df.head(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5K2wauoUH56R",
        "outputId": "a62dc893-049b-4b37-a633-a3b9defb1f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     img_path  label\n",
              "0       CASIA-WebFace_cropped/0000045/003.jpg      0\n",
              "1       CASIA-WebFace_cropped/0000045/013.jpg      0\n",
              "2       CASIA-WebFace_cropped/0000045/008.jpg      0\n",
              "3       CASIA-WebFace_cropped/0000045/011.jpg      0\n",
              "4       CASIA-WebFace_cropped/0000045/015.jpg      0\n",
              "...                                       ...    ...\n",
              "494408  CASIA-WebFace_cropped/6573530/052.jpg  10574\n",
              "494409  CASIA-WebFace_cropped/6573530/023.jpg  10574\n",
              "494410  CASIA-WebFace_cropped/6573530/026.jpg  10574\n",
              "494411  CASIA-WebFace_cropped/6573530/017.jpg  10574\n",
              "494412  CASIA-WebFace_cropped/6573530/002.jpg  10574\n",
              "\n",
              "[494413 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9be048be-4aa0-48ca-bd6d-e8615ec74c90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASIA-WebFace_cropped/0000045/003.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASIA-WebFace_cropped/0000045/013.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASIA-WebFace_cropped/0000045/008.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASIA-WebFace_cropped/0000045/011.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASIA-WebFace_cropped/0000045/015.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494408</th>\n",
              "      <td>CASIA-WebFace_cropped/6573530/052.jpg</td>\n",
              "      <td>10574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494409</th>\n",
              "      <td>CASIA-WebFace_cropped/6573530/023.jpg</td>\n",
              "      <td>10574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494410</th>\n",
              "      <td>CASIA-WebFace_cropped/6573530/026.jpg</td>\n",
              "      <td>10574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494411</th>\n",
              "      <td>CASIA-WebFace_cropped/6573530/017.jpg</td>\n",
              "      <td>10574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494412</th>\n",
              "      <td>CASIA-WebFace_cropped/6573530/002.jpg</td>\n",
              "      <td>10574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>494413 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9be048be-4aa0-48ca-bd6d-e8615ec74c90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9be048be-4aa0-48ca-bd6d-e8615ec74c90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9be048be-4aa0-48ca-bd6d-e8615ec74c90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preporcess data"
      ],
      "metadata": {
        "id": "oplRuYmhJBhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "class CASIA_Face(object):\n",
        "    def __init__(self, root_path='data_train.csv', image_size=(112, 112)):\n",
        "         self.data_path = pd.read_csv(root_path)    # Đọc file csv\n",
        "         self.num_images = len(self.data_path)      # Số row = số images\n",
        "         self.image_size = image_size               # Kích thước image input\n",
        "\n",
        "    # Hàm dùng để lấy path từ img_path của DataFrame = [\"img_path\",\"label\"]\n",
        "    def __getitem__(self, index):\n",
        "        # read image path from cvs file\n",
        "        image_path = os.path.join(self.data_path.iloc[index, 0])\n",
        "\n",
        "        # read image from image path\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        # Convert image from BGR to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize all image into same size 112x112\n",
        "        img = cv2.resize(img, self.image_size)\n",
        "\n",
        "         # read label to train cross_entropy loss\n",
        "        label_cross = self.data_path.iloc[index, 1]\n",
        "\n",
        "        # if image is gray image --> stack gray image 3 times\n",
        "        # make gray image become 3 channels\n",
        "        if len(img.shape) == 2:\n",
        "            img = np.stack([img] * 3, 2)\n",
        "\n",
        "        # Augmentation using flip technique\n",
        "        flip = np.random.choice(2)*2-1\n",
        "        img = img[:,::flip, :]\n",
        "        img = img/255\n",
        "\n",
        "        # Convert image from format [h, w, channel] into format [channel, h, w]\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        # Convert image from array to torch\n",
        "        img = torch.from_numpy(img).float()\n",
        "\n",
        "        return img, torch.from_numpy(np.array(label_cross, dtype=np.long))\n",
        "\n",
        "    # Số lượng ảnh input\n",
        "    def __len__(self):\n",
        "        return self.num_images"
      ],
      "metadata": {
        "id": "t9BPIe3B2edH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import Linear, Conv2d, BatchNorm2d, PReLU, Sequential, Module, Parameter\n",
        "# Thuật toán Norm\n",
        "def l2_norm(input, axis = 1):\n",
        "    norm = torch.norm(input, 2, axis, True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output\n",
        "\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Conv_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Conv_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "        self.prelu = PReLU(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "\n",
        "class Linear_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Linear_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "# Basic block of MobileFacenet, combining the idea from DW Conv and ResNet\n",
        "class Depth_Wise_Res(Module):\n",
        "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
        "        super(Depth_Wise_Res, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "class Multi_Res(Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
        "        super(Multi_Res, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(Depth_Wise_Res(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
        "        self.model = Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class MobileFaceNet(Module):\n",
        "    def __init__(self, embedding_size=512, class_num=1):\n",
        "        super(MobileFaceNet, self).__init__()\n",
        "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
        "        self.conv_23 = Depth_Wise_Res(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
        "        self.conv_3 = Multi_Res(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_34 = Depth_Wise_Res(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
        "        self.conv_4 = Multi_Res(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_45 = Depth_Wise_Res(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
        "        self.conv_5 = Multi_Res(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
        "       # self.adaptivePooling = AdaptiveAvgPool2d((1,1))\n",
        "        self.conv_6_flatten = Flatten()\n",
        "        self.linear = Linear(512, embedding_size, bias=False)\n",
        "           \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        out = self.conv2_dw(out)\n",
        "\n",
        "        out = self.conv_23(out)\n",
        "\n",
        "        \n",
        "        out = self.conv_3(out)\n",
        "        \n",
        "        out = self.conv_34(out)\n",
        "\n",
        "        out = self.conv_4(out)\n",
        "\n",
        "        out = self.conv_45(out)\n",
        "\n",
        "        out = self.conv_5(out)\n",
        "\n",
        "        out = self.conv_6_sep(out)\n",
        "\n",
        "        out = self.conv_6_dw(out)\n",
        "\n",
        "        out = self.conv_6_flatten(out)\n",
        "\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features=512, out_features=10575, s=32.0, m=0.50):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s  # gia tri ||s||\n",
        "        self.m = m # gia tri margin\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        \n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        # make the function cos(theta+m) monotonic decreasing while theta in [0°,180°]\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        \n",
        "        phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n",
        "\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "iCcvF2cO2fAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyper-parameter and some configurations\n",
        "BATCH_SIZE = 256\n",
        "TOTAL_EPOCH = 20\n",
        "IMG_SIZE = (112, 112)\n",
        "SAVE_DIR = '/content/drive/MyDrive/MobileFaceNet/saved_models'\n",
        "NUM_CLASSES = 10575\n",
        "\n",
        "SAVED_EPOCH = 3 # save model's weight every 5 epochs\n",
        "CASIA_DATA_DIR = './data_train.csv'\n",
        "\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "w5yZIK3n2izp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "z0CVDmoh2oob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fSTDXRzS2pKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    \n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-LbK9M92qhO",
        "outputId": "d90bc00f-67de-4561-b785-97e54720e6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# define trainloader and testloader\n",
        "trainset = CASIA_Face(root_path=CASIA_DATA_DIR, image_size = IMG_SIZE)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2, drop_last=False)\n",
        "# define model\n",
        "net = MobileFaceNet()\n",
        "ArcMargin = ArcMarginProduct(512, NUM_CLASSES)\n",
        "\n",
        "net = net.to(device)\n",
        "ArcMargin = ArcMargin.to(device)\n",
        "\n",
        "# define optimizers\n",
        "ignored_params = list(map(id, net.linear.parameters()))\n",
        "ignored_params += list(map(id, ArcMargin.weight))\n",
        "prelu_params_id = []\n",
        "prelu_params = []\n",
        "for m in net.modules():\n",
        "    if isinstance(m, nn.PReLU):\n",
        "        ignored_params += list(map(id, m.parameters()))\n",
        "        prelu_params += m.parameters()\n",
        "base_params = filter(lambda p: id(p) not in ignored_params, net.parameters())\n",
        "\n",
        "optimizer_ft = optim.SGD([\n",
        "    {'params': base_params, 'weight_decay': 4e-5},\n",
        "    {'params': net.linear.parameters(), 'weight_decay': 4e-4},\n",
        "    {'params': ArcMargin.weight, 'weight_decay': 4e-4},\n",
        "    {'params': prelu_params, 'weight_decay': 0.0}\n",
        "], lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[200000], gamma=0.1)\n",
        "\n",
        "# Using cross-entropy loss for classification human ID\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0.0\n",
        "best_epoch = 0\n",
        "iters = 0\n",
        "for epoch in range(0, TOTAL_EPOCH+1):\n",
        "    # train model\n",
        "    print('Train Epoch: {}/{} ...'.format(epoch, TOTAL_EPOCH))\n",
        "    net.train()\n",
        "\n",
        "    train_total_loss = 0.0\n",
        "    total = 0\n",
        "    since = time.time()\n",
        "    print('iteration for 1 epoch: ', iters)\n",
        "    for data in trainloader:\n",
        "        iters = iters + 1 \n",
        "        img, label = data[0].cuda(), data[1].cuda()\n",
        "        batch_size = img.size(0)\n",
        "        optimizer_ft.zero_grad()\n",
        "\n",
        "        raw_logits = net(img)\n",
        "\n",
        "        output = ArcMargin(raw_logits, label)\n",
        "        total_loss = criterion(output, label)\n",
        "        total_loss.backward()\n",
        "        optimizer_ft.step()\n",
        "        exp_lr_scheduler.step()\n",
        "\n",
        "        train_total_loss += total_loss.item() * batch_size\n",
        "        total += batch_size\n",
        "\n",
        "    train_total_loss = train_total_loss / total\n",
        "    time_elapsed = time.time() - since\n",
        "    loss_msg = 'total_loss: {:.4f} time: {:.0f}m {:.0f}s'\\\n",
        "        .format(train_total_loss, time_elapsed // 60, time_elapsed % 60)\n",
        "    print(loss_msg)\n",
        "\n",
        "    # save model\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        os.mkdir(SAVE_DIR)\n",
        "    if epoch % SAVED_EPOCH == 0:\n",
        "        torch.save(net.state_dict(),\n",
        "            os.path.join(SAVE_DIR, '%03d.pth' % epoch))\n",
        "print('finishing training')\n"
      ],
      "metadata": {
        "id": "NO8ikLJG2rxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQZDrV0a2tgN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}